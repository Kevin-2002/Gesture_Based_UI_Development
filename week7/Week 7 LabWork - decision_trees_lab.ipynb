{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de76dd25",
   "metadata": {},
   "source": [
    "# Lab: Decision Trees and Random Forests\n",
    "\n",
    "## Objectives\n",
    "By the end of this lab, you will:\n",
    "- Understand how decision trees work and their limitations.\n",
    "- Observe the instability of decision trees.\n",
    "- Learn how random forests improve stability and accuracy.\n",
    "- Implement both models using `scikit-learn`.\n",
    "- Evaluate performance using cross-validation and feature importance.\n",
    "- Use cross-validation for hyperparameter selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37384b0c",
   "metadata": {},
   "source": [
    "## **1. Introduction**\n",
    "Decision Trees are powerful models but can be unstable, meaning small variations in data can lead to significantly different trees. Random Forests, an ensemble method, mitigate this issue by averaging multiple decision trees trained on random subsets of data.\n",
    "\n",
    "### **Dataset: Banknote Authentication**\n",
    "For this lab, we will use the **Banknote Authentication Dataset**, which contains features extracted from images of banknotes to determine if they are genuine or forged.\n",
    "- Available on Kaggle: [Banknote Authentication Dataset](https://www.kaggle.com/datasets/ritesaluja/bank-note-authentication-uci-data)\n",
    "- Features:\n",
    "  - Variance of wavelet transformed image\n",
    "  - Skewness of wavelet transformed image\n",
    "  - Kurtosis of wavelet transformed image\n",
    "  - Entropy of the image\n",
    "  - Class label: (0 = Forged, 1 = Genuine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f88a3a-683d-48c6-b7ae-8a2da63ba70f",
   "metadata": {},
   "source": [
    "# 2. Load and Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d669b79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"/path/to/data.csv\")  # Replace with actual path\n",
    "\n",
    "# Check the first few rows\n",
    "display(df.head())\n",
    "\n",
    "# Check class distribution\n",
    "print(df[\"Class\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9375eae-fbd5-4bcd-978d-d393e79b396b",
   "metadata": {},
   "source": [
    "# 3. Train a Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11342834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split data\n",
    "X = df.drop(columns=[\"Class\"])\n",
    "y = df[\"Class\"]\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Decision Tree model\n",
    "# TODO: Initialize and fit a DecisionTreeClassifier\n",
    "\n",
    "# Evaluate performance\n",
    "# TODO: Predict on X_valid and calculate accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a0acfd-d713-4b3b-8e09-4cd741803f0c",
   "metadata": {},
   "source": [
    "# 4. Demonstrate Decision Tree Instability\n",
    "To show instability, we train multiple trees with different subsets of data and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf58c0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Train multiple trees with different random states\n",
    "scores = [DecisionTreeClassifier(random_state=i).fit(X_train, y_train).score(X_valid, y_valid) for i in range(10, 110, 10)]\n",
    "\n",
    "# Plot the accuracy variation\n",
    "plt.plot(range(10, 110, 10), scores, marker='o', linestyle='--', color='b')\n",
    "plt.xlabel(\"Random State\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Decision Tree Instability with Different Random States\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeb8dc2-41ff-4726-bdb4-e4abc7675547",
   "metadata": {},
   "source": [
    "# 5. Train a Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061193d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train Random Forest model\n",
    "# TODO: Initialize and fit a RandomForestClassifier\n",
    "\n",
    "# Evaluate performance\n",
    "# TODO: Predict on X_valid and calculate accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5905ed76-2324-43bf-bf49-de3e0f614df0",
   "metadata": {},
   "source": [
    "# 6. Cross-Validation for Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d4024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'n_estimators': [50, 100, 200]\n",
    "}\n",
    "\n",
    "# TODO: Perform GridSearchCV with RandomForestClassifier\n",
    "\n",
    "# Print best parameters\n",
    "# TODO: Print best parameters and best score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a44f053-4bd5-463d-87bd-115a1b293bfc",
   "metadata": {},
   "source": [
    "# 7. Compare Decision Tree vs Random Forest Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef5af4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_scores = cross_val_score(DecisionTreeClassifier(random_state=42), X, y, cv=5)\n",
    "rf_scores = cross_val_score(RandomForestClassifier(n_estimators=100, random_state=42), X, y, cv=5)\n",
    "\n",
    "print(f\"Decision Tree Cross-Val Accuracy: {dt_scores.mean():.4f} ± {dt_scores.std():.4f}\")\n",
    "print(f\"Random Forest Cross-Val Accuracy: {rf_scores.mean():.4f} ± {rf_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8240eb5d-3809-4ee6-a327-404a9cd77749",
   "metadata": {},
   "source": [
    "# 8. Feature Importance in Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493ba140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance\n",
    "importances = rf.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(feature_names, importances, color='green')\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance in Random Forest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49143e9e",
   "metadata": {},
   "source": [
    "## **9. Conclusion**\n",
    "- **Decision Trees** are simple but unstable, leading to variance in performance.\n",
    "- **Random Forests** improve stability and accuracy by averaging predictions from multiple trees.\n",
    "- Feature importance in Random Forests helps identify the most influential features.\n",
    "- Cross-validation helps tune hyperparameters for better model performance.\n",
    "\n",
    "### **Further Exploration**\n",
    "- Try different `max_depth` or `min_samples_split` values in `DecisionTreeClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553c733e-5883-4220-b2e5-11a9f1143533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa1ad07-3029-4026-92e7-093e9dc43c75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ee3ecc-0927-4c50-9d9c-7d39f1b1f59f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dce9c45-73bd-4314-82f6-b833752d2800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b177f5a-79e4-4f97-8cf9-ca56cf590009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa3c6b7e-5524-47ab-9df9-8c52730d5afe",
   "metadata": {},
   "source": [
    "# 10. Go Back to Other Labs\n",
    "\n",
    "See how RandomForests do with the Breast Cancer Data Set and with the Colour Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ff1275-1790-4e43-88d0-e14c13ab38f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
